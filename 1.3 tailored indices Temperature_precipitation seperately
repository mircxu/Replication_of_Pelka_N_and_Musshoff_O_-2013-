# Step 1: Load Data
final_data_with_station_IDs <- read.csv("final_data_with_station_IDs.csv")
all_temp_results <- read.csv("all_temp_results.csv", header = TRUE, check.names = FALSE)

# Step 2: Prepare Temperature Data
all_temp_results_melted <- all_temp_results %>%
  gather(key = "stations_id", value = "IT", -year, -period) %>%
  mutate(
    stations_id = as.numeric(as.character(stations_id)),
    period = as.character(period)
  )

# Ensure consistent column types in the final dataset
final_data_with_station_IDs <- final_data_with_station_IDs %>%
  mutate(
    year = as.numeric(year),
    stations_id = as.numeric(stations_id)
  )

# Define Accumulation Periods
periods <- c("3:3", "3:4", "3:5", "3:6", "3:7", "3:8", "3:9", "3:10")

# Function to Optimize Accumulation Period Based on Quadratic Model Fit (R-squared)
optimize_accumulation_period_model_based <- function(data, temp_results) {
  best_period <- NA
  best_r_squared <- -Inf  # Initialize to track best R-squared
  
  for (period in periods) {
    # Get the data for the given period
    temp_period <- temp_results %>% filter(period == !!period)
    
    # Merge temperature data with district data for the period
    merged_data <- data %>%
      left_join(temp_period, by = c("year", "stations_id")) %>%
      filter(!is.na(IT), !is.na(value))  # Filter out NA values for IT and yield
    
    if (nrow(merged_data) > 0) {
      # Fit the quadratic model (Y_t ~ IT + I(IT^2))
      model <- lm(value ~ IT + I(IT^2), data = merged_data)
      
      # Get the R-squared value of the model
      r_squared <- summary(model)$r.squared
      
      # If the current model has a better R-squared, select it
      if (!is.na(r_squared) && r_squared > best_r_squared) {
        best_r_squared <- r_squared
        best_period <- period
      }
    }
  }
  
  return(best_period)
}

# Step 3: Optimize Periods for Each District Based on Model Fit
optimized_periods <- final_data_with_station_IDs %>%
  group_split(district_no) %>%
  map_df(~ {
    district_data <- .x
    optimal_period <- optimize_accumulation_period_model_based(district_data, all_temp_results_melted)
    tibble(district_no = unique(district_data$district_no), optimal_period = optimal_period)
  })

# Step 4: Merge Data with Optimized Periods
final_merged_data <- final_data_with_station_IDs %>%
  left_join(optimized_periods, by = "district_no") %>%
  left_join(all_temp_results_melted, by = c("year", "stations_id", "optimal_period" = "period")) %>%
  rename(Yield = value)

# Step 5: Fit the Quadratic Model for Each District
models_stations <- final_merged_data %>%
  group_by(district_no, optimal_period) %>%
  do({
    data <- .
    if (nrow(data) > 0) {
      # Fit quadratic model Y_t ~ IT + I(IT^2)
      model <- lm(Yield ~ IT + I(IT^2), data = data)
      tidy(model) %>%
        mutate(district_no = unique(data$district_no))
    } else {
      tibble()
    }
  }) %>%
  ungroup()

# Save model coefficients
write.csv(models_stations, 'models_stations_coefficients.csv', row.names = FALSE)

# Step 6: Add Predictions
models_stations_wider <- models_stations %>%
  select(term, estimate, district_no) %>%
  pivot_wider(names_from = term, values_from = estimate) %>%
  rename(
    Intercept_coef = `(Intercept)`,
    IT_coef = `IT`,
    IT_squared_coef = `I(IT^2)`
  )

df_with_predictions <- final_merged_data %>%
  left_join(models_stations_wider, by = "district_no") %>%
  mutate(
    prediction = Intercept_coef + IT_coef * IT + IT_squared_coef * IT^2
  )

# Save data with predictions
write.csv(df_with_predictions, "district_predictions.csv", row.names = FALSE)

# Step 7: Calculate Strike Levels and Call Option Payouts
df_with_predictions <- df_with_predictions %>%
  group_by(district_no) %>%
  mutate(strike_level_IT = mean(IT, na.rm = TRUE)) %>%
  rowwise() %>%
  mutate(n_call_IT = max(IT - strike_level_IT, 0)) %>%
  ungroup()

# Save call option results
write.csv(df_with_predictions, "temperature_call_options.csv", row.names = FALSE)

# Step 8: Bootstrap for Fair Premium Calculation
n_iterations <- 10000
r <- 0.05
years <- 15

mean_func <- function(data_, indices) {
  return(mean(data_[indices], na.rm = TRUE))
}

bootstrap_results <- boot(df_with_predictions$n_call_IT, statistic = mean_func, R = n_iterations)
bootstrap_means <- bootstrap_results$t
expected_payout <- mean(bootstrap_means)
fair_premium <- expected_payout / (1 + r)^years

cat("Fair Premium (Temperature Call Option):", fair_premium, "\n")

# Step 9: Calculate Hedging Effectiveness
df_with_predictions <- df_with_predictions %>%
  mutate(yt_price = Yield * 16) %>%
  group_by(district_no) %>%
  mutate(z = -cov(yt_price, n_call_IT) / var(n_call_IT)) %>%
  ungroup() %>%
  mutate(revenue = yt_price + z * n_call_IT - z * fair_premium)

std_without_call <- aggregate(yt_price ~ district_no, df_with_predictions, sd)
std_with_call <- aggregate(revenue ~ district_no, df_with_predictions, sd)

results_call <- merge(std_without_call, std_with_call, by = "district_no")
names(results_call) <- c("district", "Std_Without", "Std_With")
results_call$Hedging_Effectiveness <- (results_call$Std_Without - results_call$Std_With) / results_call$Std_Without * 100

# Save hedging effectiveness results
write.csv(results_call, "hedging_effectiveness_Temperature.csv", row.names = FALSE)
# Step 1: Load Data
final_data_with_station_IDs <- read.csv("final_data_with_station_IDs.csv")
all_temp_results <- read.csv("all_temp_results.csv", header = TRUE, check.names = FALSE)

# Step 2: Prepare Temperature Data
all_temp_results_melted <- all_temp_results %>%
  gather(key = "stations_id", value = "IT", -year, -period) %>%
  mutate(
    stations_id = as.numeric(as.character(stations_id)),
    period = as.character(period)
  )

# Ensure consistent column types in the final dataset
final_data_with_station_IDs <- final_data_with_station_IDs %>%
  mutate(
    year = as.numeric(year),
    stations_id = as.numeric(stations_id)
  )

# Define Accumulation Periods
periods <- c("3:3", "3:4", "3:5", "3:6", "3:7", "3:8", "3:9", "3:10")

# Function to Optimize Accumulation Period Based on Quadratic Model Fit (R-squared)
optimize_accumulation_period_model_based <- function(data, temp_results) {
  best_period <- NA
  best_r_squared <- -Inf  # Initialize to track best R-squared
  
  for (period in periods) {
    # Get the data for the given period
    temp_period <- temp_results %>% filter(period == !!period)
    
    # Merge temperature data with district data for the period
    merged_data <- data %>%
      left_join(temp_period, by = c("year", "stations_id")) %>%
      filter(!is.na(IT), !is.na(value))  # Filter out NA values for IT and yield
    
    if (nrow(merged_data) > 0) {
      # Fit the quadratic model (Y_t ~ IT + I(IT^2))
      model <- lm(value ~ IT + I(IT^2), data = merged_data)
      
      # Get the R-squared value of the model
      r_squared <- summary(model)$r.squared
      
      # If the current model has a better R-squared, select it
      if (!is.na(r_squared) && r_squared > best_r_squared) {
        best_r_squared <- r_squared
        best_period <- period
      }
    }
  }
  
  return(best_period)
}

# Step 3: Optimize Periods for Each District Based on Model Fit
optimized_periods <- final_data_with_station_IDs %>%
  group_split(district_no) %>%
  map_df(~ {
    district_data <- .x
    optimal_period <- optimize_accumulation_period_model_based(district_data, all_temp_results_melted)
    tibble(district_no = unique(district_data$district_no), optimal_period = optimal_period)
  })

# Step 4: Merge Data with Optimized Periods
final_merged_data <- final_data_with_station_IDs %>%
  left_join(optimized_periods, by = "district_no") %>%
  left_join(all_temp_results_melted, by = c("year", "stations_id", "optimal_period" = "period")) %>%
  rename(Yield = value)

# Step 5: Fit the Quadratic Model for Each District
models_stations <- final_merged_data %>%
  group_by(district_no, optimal_period) %>%
  do({
    data <- .
    if (nrow(data) > 0) {
      # Fit quadratic model Y_t ~ IT + I(IT^2)
      model <- lm(Yield ~ IT + I(IT^2), data = data)
      tidy(model) %>%
        mutate(district_no = unique(data$district_no))
    } else {
      tibble()
    }
  }) %>%
  ungroup()

# Save model coefficients
write.csv(models_stations, 'models_stations_coefficients.csv', row.names = FALSE)

# Step 6: Add Predictions
models_stations_wider <- models_stations %>%
  select(term, estimate, district_no) %>%
  pivot_wider(names_from = term, values_from = estimate) %>%
  rename(
    Intercept_coef = `(Intercept)`,
    IT_coef = `IT`,
    IT_squared_coef = `I(IT^2)`
  )

df_with_predictions <- final_merged_data %>%
  left_join(models_stations_wider, by = "district_no") %>%
  mutate(
    prediction = Intercept_coef + IT_coef * IT + IT_squared_coef * IT^2
  )

# Save data with predictions
write.csv(df_with_predictions, "district_predictions.csv", row.names = FALSE)

# Step 7: Calculate Strike Levels and Call Option Payouts
df_with_predictions <- df_with_predictions %>%
  group_by(district_no) %>%
  mutate(strike_level_IT = mean(IT, na.rm = TRUE)) %>%
  rowwise() %>%
  mutate(n_call_IT = max(IT - strike_level_IT, 0)) %>%
  ungroup()

# Save call option results
write.csv(df_with_predictions, "temperature_call_options.csv", row.names = FALSE)

# Step 8: Bootstrap for Fair Premium Calculation
n_iterations <- 10000
r <- 0.05
years <- 16

mean_func <- function(data_, indices) {
  return(mean(data_[indices], na.rm = TRUE))
}

bootstrap_results <- boot(df_with_predictions$n_call_IT, statistic = mean_func, R = n_iterations)
bootstrap_means <- bootstrap_results$t
expected_payout <- mean(bootstrap_means)
fair_premium <- expected_payout / (1 + r)^years

cat("Fair Premium (Temperature Call Option):", fair_premium, "\n")

# Step 9: Calculate Hedging Effectiveness
df_with_predictions <- df_with_predictions %>%
  mutate(yt_price = Yield * 16) %>%
  group_by(district_no) %>%
  mutate(z = -cov(yt_price, n_call_IT) / var(n_call_IT)) %>%
  ungroup() %>%
  mutate(revenue = yt_price + z * n_call_IT - z * fair_premium)

std_without_call <- aggregate(yt_price ~ district_no, df_with_predictions, sd)
std_with_call <- aggregate(revenue ~ district_no, df_with_predictions, sd)

results_call <- merge(std_without_call, std_with_call, by = "district_no")
names(results_call) <- c("district", "Std_Without", "Std_With")
results_call$Hedging_Effectiveness <- (results_call$Std_Without - results_call$Std_With) / results_call$Std_Without * 100

# Save hedging effectiveness results
write.csv(results_call, "hedging_effectiveness_Temperature.csv", row.names = FALSE)


#Display the results
cat("\nHedging Effectiveness (Temperature):\n")
print(hedging_temp)

cat("\nPaired t-test Results (Temperature):\n")
cat("t-value:", t_test_temp$statistic, "\n")
cat("p-value:", t_test_temp$p.value, "\n")
cat("95% Confidence Interval:", t_test_temp$conf.int, "\n")
cat("Mean Difference:", t_test_temp$estimate, "\n")


# Step 1: Load Data
final_data_with_station_IDs <- read.csv("final_data_with_station_IDs.csv")
all_precip_results <- read.csv("all_rain_results.csv", header = TRUE, check.names = FALSE)

# Step 2: Prepare Precipitation Data
all_precip_results_melted <- all_precip_results %>%
  gather(key = "stations_id", value = "PR", -year, -period) %>%
  mutate(
    stations_id = as.numeric(as.character(stations_id)),
    period = as.character(period)
  )

# Ensure consistent column types in the final dataset
final_data_with_station_IDs <- final_data_with_station_IDs %>%
  mutate(
    year = as.numeric(year),
    stations_id = as.numeric(stations_id)
  )

# Define Accumulation Periods
periods <- c("3:3", "3:4", "3:5", "3:6", "3:7", "3:8", "3:9", "3:10")

# Function to Optimize Accumulation Period Based on Quadratic Model Fit (R-squared)
optimize_accumulation_period_model_based <- function(data, precip_results) {
  best_period <- NA
  best_r_squared <- -Inf  # Initialize to track best R-squared
  
  for (period in periods) {
    # Get the data for the given period
    precip_period <- precip_results %>% filter(period == !!period)
    
    # Merge precipitation data with district data for the period
    merged_data <- data %>%
      left_join(precip_period, by = c("year", "stations_id")) %>%
      filter(!is.na(PR), !is.na(value))  # Filter out NA values for PR and yield
    
    if (nrow(merged_data) > 0) {
      # Fit the quadratic model (Y_t ~ PR + I(PR^2))
      model <- lm(value ~ PR + I(PR^2), data = merged_data)
      
      # Get the R-squared value of the model
      r_squared <- summary(model)$r.squared
      
      # If the current model has a better R-squared, select it
      if (!is.na(r_squared) && r_squared > best_r_squared) {
        best_r_squared <- r_squared
        best_period <- period
      }
    }
  }
  
  return(best_period)
}

# Step 3: Optimize Periods for Each District Based on Model Fit
optimized_periods <- final_data_with_station_IDs %>%
  group_split(district_no) %>%
  map_df(~ {
    district_data <- .x
    optimal_period <- optimize_accumulation_period_model_based(district_data, all_precip_results_melted)
    tibble(district_no = unique(district_data$district_no), optimal_period = optimal_period)
  })

# Step 4: Merge Data with Optimized Periods
final_merged_data <- final_data_with_station_IDs %>%
  left_join(optimized_periods, by = "district_no") %>%
  left_join(all_precip_results_melted, by = c("year", "stations_id", "optimal_period" = "period")) %>%
  rename(Yield = value)

# Step 5: Fit the Quadratic Model for Each District
models_stations <- final_merged_data %>%
  group_by(district_no, optimal_period) %>%
  do({
    data <- .
    if (nrow(data) > 0) {
      # Fit quadratic model Y_t ~ PR + I(PR^2)
      model <- lm(Yield ~ PR + I(PR^2), data = data)
      tidy(model) %>%
        mutate(district_no = unique(data$district_no))
    } else {
      tibble()
    }
  }) %>%
  ungroup()

# Save model coefficients
write.csv(models_stations, 'models_stations_coefficients.csv', row.names = FALSE)

# Step 6: Add Predictions
models_stations_wider <- models_stations %>%
  select(term, estimate, district_no) %>%
  pivot_wider(names_from = term, values_from = estimate) %>%
  rename(
    Intercept_coef = `(Intercept)`,
    PR_coef = `PR`,
    PR_squared_coef = `I(PR^2)`
  )

df_with_predictions <- final_merged_data %>%
  left_join(models_stations_wider, by = "district_no") %>%
  mutate(
    prediction = Intercept_coef + PR_coef * PR + PR_squared_coef * PR^2
  )

# Save data with predictions
write.csv(df_with_predictions, "district_predictions.csv", row.names = FALSE)

# Step 7: Calculate Strike Levels and Call Option Payouts
df_with_predictions <- df_with_predictions %>%
  group_by(district_no) %>%
  mutate(strike_level_PR = mean(PR, na.rm = TRUE)) %>%
  rowwise() %>%
  mutate(n_call_PR = max(PR - strike_level_PR, 0)) %>%
  ungroup()

# Save call option results
write.csv(df_with_predictions, "precipitation_call_options.csv", row.names = FALSE)

# Step 8: Bootstrap for Fair Premium Calculation
n_iterations <- 10000
r <- 0.05
years <- 16

mean_func <- function(data_, indices) {
  return(mean(data_[indices], na.rm = TRUE))
}

bootstrap_results <- boot(df_with_predictions$n_call_PR, statistic = mean_func, R = n_iterations)
bootstrap_means <- bootstrap_results$t
expected_payout <- mean(bootstrap_means)
fair_premium <- expected_payout / (1 + r)^years

cat("Fair Premium (Precipitation Call Option):", fair_premium, "\n")

# Step 9: Calculate Hedging Effectiveness
df_with_predictions <- df_with_predictions %>%
  mutate(yt_price = Yield * 16) %>%
  group_by(district_no) %>%
  mutate(z = -cov(yt_price, n_call_PR) / var(n_call_PR)) %>%
  ungroup() %>%
  mutate(revenue = yt_price + z * n_call_PR - z * fair_premium)

std_without_call <- aggregate(yt_price ~ district_no, df_with_predictions, sd)
std_with_call <- aggregate(revenue ~ district_no, df_with_predictions, sd)

results_call <- merge(std_without_call, std_with_call, by = "district_no")
names(results_call) <- c("district", "Std_Without", "Std_With")
results_call$Hedging_Effectiveness <- (results_call$Std_Without - results_call$Std_With) / results_call$Std_Without * 100

# Save hedging effectiveness results
write.csv(results_call, "hedging_effectiveness_Precipitation.csv", row.names = FALSE)


