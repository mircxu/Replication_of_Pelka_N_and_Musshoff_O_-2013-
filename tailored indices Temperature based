# Load necessary libraries
library(tidyverse)
library(lubridate)
library(broom)
library(boot)

# Step 1: Load Data
final_data_with_station_IDs <- read.csv("final_data_with_station_IDs.csv")
all_temp_results <- read.csv("all_temp_results.csv", header = TRUE, check.names = FALSE)

# Step 2: Prepare Temperature Data
all_temp_results_melted <- all_temp_results %>%
  gather(key = "stations_id", value = "IT", -year, -period) %>%
  mutate(
    stations_id = as.numeric(as.character(stations_id)),
    period = as.character(period)
  )

# Ensure consistent column types
final_data_with_station_IDs <- final_data_with_station_IDs %>%
  mutate(
    year = as.numeric(year),
    stations_id = as.numeric(stations_id)
  )

# Step 3: Define Accumulation Period Optimization
periods <- c("3:3", "3:4", "3:5", "3:6", "3:7", "3:8", "3:9", "3:10")

optimize_accumulation_period <- function(data, temp_results) {
  best_period <- NA
  best_corr <- -Inf
  
  for (period in periods) {
    temp_period <- temp_results %>% filter(period == !!period)
    merged_data <- data %>%
      left_join(temp_period, by = c("year", "stations_id")) %>%
      filter(!is.na(IT), !is.na(value))
    
    if (nrow(merged_data) > 5) {  # Ensure enough data points for correlation
      corr <- cor(merged_data$value, merged_data$IT, use = "complete.obs")
      if (!is.na(corr) && corr > best_corr) {
        best_corr <- corr
        best_period <- period
      }
    }
  }
  
  return(best_period)
}

# Step 4: Optimize Periods for Each District
optimized_periods <- final_data_with_station_IDs %>%
  group_split(district_no) %>%  # Split by district_no
  map_df(~ {
    district_data <- .x
    optimal_period <- optimize_accumulation_period(district_data, all_temp_results_melted)
    tibble(district_no = unique(district_data$district_no), optimal_period = optimal_period)
  })

# Step 5: Merge with Optimized Periods
final_merged_data <- final_data_with_station_IDs %>%
  left_join(optimized_periods, by = "district_no") %>%
  left_join(all_temp_results_melted, by = c("year", "stations_id", "optimal_period" = "period")) %>%
  rename(Yield = value)

# Step 6: Fit Quadratic Models for Each District
models_stations <- final_merged_data %>%
  group_by(district_no, optimal_period) %>%
  do({
    data <- .
    if (nrow(data) > 0) {
      model <- lm(Yield ~ IT + I(IT^2), data = data)
      tidy(model) %>%
        mutate(district_no = unique(data$district_no))
    } else {
      tibble()
    }
  }) %>%
  ungroup()

# Save model coefficients
write.csv(models_stations, 'models_stations_coefficients.csv', row.names = FALSE)

# Step 7: Add Predictions
models_stations_wider <- models_stations %>%
  select(term, estimate, district_no) %>%
  pivot_wider(names_from = term, values_from = estimate) %>%
  rename(
    Intercept_coef = `(Intercept)`,
    IT_coef = `IT`,
    IT_squared_coef = `I(IT^2)`
  )

df_with_predictions <- final_merged_data %>%
  left_join(models_stations_wider, by = "district_no") %>%
  mutate(
    prediction = Intercept_coef + IT_coef * IT + IT_squared_coef * (IT^2)
  )

# Save data with predictions
write.csv(df_with_predictions, "district_predictions.csv", row.names = FALSE)

# Step 8: Calculate Strike Levels and Call Option Payouts
df_with_predictions <- df_with_predictions %>%
  group_by(district_no) %>%
  mutate(strike_level_IT = mean(IT, na.rm = TRUE)) %>%
  rowwise() %>%
  mutate(n_call_IT = max(IT - strike_level_IT, 0)) %>%
  ungroup()

# Save call option results
write.csv(df_with_predictions, "temperature_call_options.csv", row.names = FALSE)

# Step 9: Bootstrap for Fair Premium
n_iterations <- 10000
r <- 0.05
years <- 15

mean_func <- function(data_, indices) {
  return(mean(data_[indices], na.rm = TRUE))
}

bootstrap_results <- boot(df_with_predictions$n_call_IT, statistic = mean_func, R = n_iterations)
bootstrap_means <- bootstrap_results$t
expected_payout <- mean(bootstrap_means)
fair_premium <- expected_payout / (1 + r)^years

cat("Fair Premium (Temperature Call Option):", fair_premium, "\n")

# Step 10: Hedging Effectiveness
df_with_predictions <- df_with_predictions %>%
  mutate(yt_price = Yield * 16) %>%
  group_by(district_no) %>%
  mutate(z = -cov(yt_price, n_call_IT) / var(n_call_IT)) %>%
  ungroup() %>%
  mutate(revenue = yt_price + z * n_call_IT - z * fair_premium)

std_without_call <- aggregate(yt_price ~ district_no, df_with_predictions, sd)
std_with_call <- aggregate(revenue ~ district_no, df_with_predictions, sd)

results_call <- merge(std_without_call, std_with_call, by = "district_no")
names(results_call) <- c("district", "Std_Without", "Std_With")
results_call$Hedging_Effectiveness <- (results_call$Std_Without - results_call$Std_With) / results_call$Std_Without * 100

# Save hedging effectiveness results
write.csv(results_call, "hedging_effectiveness_temperature.csv", row.names = FALSE)

# Step 11: T-Test for Hedging Effectiveness
t_test_results <- t.test(df_with_predictions$yt_price, df_with_predictions$revenue, paired = TRUE)

cat("Paired t-test Results:\n")
cat("t-value:", t_test_results$statistic, "\n")
cat("p-value:", t_test_results$p.value, "\n")
cat("Confidence Interval (95%):", t_test_results$conf.int, "\n")
